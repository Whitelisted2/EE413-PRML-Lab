{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zZDMbDP0uJN0"
      },
      "source": [
        "# **LAB 10 : Hidden Markov Model**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Terms\n",
        "- <b>State Transition Probabilities</b> determine the probabilities of each future state, given the current state.\n",
        "- <b>Observation Symbol (Emission) Probabilities</b> determine the probabilities of observations emitted by the current state of the system.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "MKFudLjzt_K5"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "64j_rFZeF57i"
      },
      "source": [
        "Please refer to the following [article](http://www.adeveloperdiary.com/data-science/machine-learning/introduction-to-hidden-markov-model/) to understand Hidden Markov Model\n",
        "\n",
        "Here we will be dealing with 3 major problems : \n",
        "  \n",
        "  1. Evaluation Problem\n",
        "  2. Learning Problem\n",
        "  3. Decoding Problem"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wEasU5TNGNbE"
      },
      "source": [
        "### 1. Evaluation Problem : Implementation of Forward and Backward Algorithm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Goal: \n",
        "    $$P(V^{T}|\\lambda)$$\n",
        "\n",
        "The forward variable $\\alpha_{t}(i)$ is defined as the probability of the partial observation sequence $v_{1}$,$v_{2}$,...,$v_{t}$, when it terminates at the state i. Mathematically, \n",
        "\\begin{align*}\n",
        "\\alpha_{t}(i) & = p(v_{1},v_{2},...,v_{t}, q_{t} = i | \\lambda) \\\\ \n",
        "\\alpha_{t+1}(i) & = b_{j}(v_{t+1}) \\sum_{i=1}^{N} \\alpha_{t}(i)a_{ij} ~~~~~~~~~~~   ,1\\leq j \\leq N, 1 \\leq t \\leq T-1 \\\\\n",
        "\\alpha_{1}(j) & = \\pi_{j}b_{j}(v_{1})   ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ,1\\leq j \\leq N\n",
        "\\end{align*}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vjUe9joAGXvC",
        "outputId": "0a1ef95b-9653-4518-adbc-d288b6343da5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "    Hidden  Visible\n",
            "0        B        0\n",
            "1        B        1\n",
            "2        B        2\n",
            "3        B        2\n",
            "4        B        2\n",
            "..     ...      ...\n",
            "495      A        1\n",
            "496      A        2\n",
            "497      B        2\n",
            "498      A        1\n",
            "499      A        2\n",
            "\n",
            "[500 rows x 2 columns]\n"
          ]
        }
      ],
      "source": [
        "data = pd.read_csv('data_python.csv') ## Read the data, change the path accordingly\n",
        "print(data)\n",
        "V = data['Visible'].values # get only observable data\n",
        "\n",
        "\n",
        "# Transition Probabilities\n",
        "a = np.array(((0.54, 0.46), (0.49, 0.51)))                  # a: N x N = 2 x 2\n",
        "\n",
        "# Emission Probabilities\n",
        "b = np.array(((0.16, 0.26, 0.58),\n",
        "              (0.25, 0.28, 0.47)))                          # b: N x M = 2 x 3\n",
        "\n",
        "# Equal Probabilities for the initial distribution\n",
        "initial_distribution = np.array((0.5, 0.5))                 # pi: 1 x N = 1 x 2 ; this is pi\n",
        "\n",
        "\n",
        "def forward(V, a, b, initial_distribution):\n",
        "    T = V.shape[0]\n",
        "    N = a.shape[0]\n",
        "    alpha = np.zeros((T, N))                                # T = num of obs, N = num of states\n",
        "    alpha[0, :] = initial_distribution * b[:, V[0]]         # base case\n",
        "    for t in range(1, T):\n",
        "        for j in range(N):\n",
        "            alpha[t, j] = np.dot(alpha[t-1, :], a[:,j]) * b[j, V[t]] # recursively get alpha for time t until t=T-1\n",
        "    # print(alpha)\n",
        "    return alpha\n",
        "\n",
        "alpha = forward(V, a, b, initial_distribution)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In a similar way we can define the backward variable $\\beta_{t}(i)$ as the probability of the partial observation sequence $v_{t+1}$,$v_{t+2}$,...,$v_{T}$ , given that the current state is i. Mathematically,\n",
        "\n",
        "\\begin{align*}\n",
        "\\beta_{t}(i) & = p(v_{t+1},v_{t+2},...,v_{T} | q_{t} = i, \\lambda) \\\\ \n",
        "\\beta_{t}(i) & = \\sum_{j=1}^{N} \\beta_{t+1}(j) a_{ij} b_{j}(v_{t+1})~~~~~~~~~~~~~~~,1\\leq i \\leq N, 1 \\leq t \\leq T-1 \\\\\n",
        "\\beta_{T}(i) & = 1~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~,1\\leq i \\leq N\n",
        "\\end{align*}\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "def backward(V, a, b):\n",
        "    T = V.shape[0]\n",
        "    N = a.shape[0]\n",
        "    beta = np.zeros((T, N))\n",
        "    beta[T-1, :] = np.ones((N))         # set last row to ones\n",
        "    for t in range(T-2, -1, -1):        # T-1 to 0 (non-incl), steps of -1\n",
        "        for i in range(a.shape[0]):\n",
        "            beta[t, i] = np.dot(beta[t+1]*b[:, V[t+1]], a[i, :])\n",
        "    return beta\n",
        "\n",
        "beta = backward(V, a, b)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Iyc4mIlGzPk"
      },
      "source": [
        "### 2. Learning Problem : Implementation of Baum Welch Algorithm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Goal: ${\\lambda , B, \\pi}$ in order to maximize $P(V^{T}|\\lambda)$\n",
        "\n",
        "Let us define a new auxiliary variable $\\xi_{t}(i,j)$ as the probability of being in state i at t=t and in state j at t=t+1. Then,\n",
        "\n",
        "\\begin{align*}\n",
        "\\xi_{t}(i,j) & = p(q_{t} = i, q_{t+1} = j | V^{T}, \\lambda) \\\\\n",
        " & = \\frac{p(q_{t} = i, q_{t+1} = j, V^{T} | \\lambda)}{p(V^{T}|\\lambda)} \\\\\n",
        " & = \\frac{\\alpha_{t}(i)a_{ij}\\beta_{t+1}(j)b_{j}(v_{t+1})}{\\sum_{i=1}^{N} \\sum_{j=1}^{N} \\alpha_{t}(i)a_{ij}\\beta_{t+1}(j)b_{j}(v_{t+1})}\n",
        "\\end{align*}\n",
        "\n",
        "Also, let us define $\\gamma_{t}(i)$ as the a poteriori probability, that is the probability of being in state i at t=t, given the observation sequence and the model.\n",
        "\n",
        "\\begin{align*}\n",
        "\\gamma_{t}(i) & = p(q_{t} | V^{T}, \\lambda) \\\\\n",
        "& = \\frac{\\alpha_{t}(i)\\beta_{t}(i)}{\\sum_{i=1}^{N} \\alpha_{t}(i)\\beta_{t}(i)}\n",
        "\\end{align*}\n",
        "\n",
        "Also, \n",
        "$$ \\gamma_{t}(i) = \\sum_{j=1}^{N} \\xi_{t}(i,j)  ~~~~~~~, 1\\leq j \\leq N $$\n",
        "\n",
        "Our goal:\n",
        "\\begin{align*}\n",
        "\\overline{a}_{ij} &= \\frac{\\text{expected number of transitions from hidden state i to state j}}{\\text{expected number of transition from hidden state i}} \\\\\n",
        "\\overline{b}_{j}(k) &= \\frac{\\text{expected number of times in hidden state j and observing v(k)}}{\\text{expected number of times in hidden state j}}\n",
        "\\end{align*}\n",
        "\n",
        "Re-estimations are:\n",
        "\\begin{align*}\n",
        "\\overline{a}_{ij} &= \\frac{\\sum_{t=1}^{T-1} \\xi_{t}(i, j)}{\\sum_{t=1}^{T-1} \\gamma_{t}(i)} ~~~~~, 1\\leq i \\leq N, 1\\leq j \\leq N \\\\\n",
        "\\overline{b}_{j}(k) &= \\frac{\\sum_{t=1}^{T} \\gamma_{t}(j) 1(v_{t} == k)}{\\sum_{t=1}^{T} \\gamma_{t}(j)} ~~~~~, 1\\leq j \\leq N, 1\\leq k \\leq M\n",
        "\\end{align*}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "def baum_welch(V, a, b, initial_distribution, n_iter=100):\n",
        "    T = V.shape[0]\n",
        "    N = a.shape[0]\n",
        "    M = b.shape[1]\n",
        "    \n",
        "    for it in range(n_iter):\n",
        "        alpha = forward(V, a, b, initial_distribution)  # 500 x 2\n",
        "        beta = backward(V, a, b)                        # 500 x 2\n",
        "        \n",
        "        xi = np.zeros((N, N, T-1))\n",
        "        \n",
        "        for t in range(T-1):\n",
        "            d = np.dot(np.dot(alpha[t,:].T, a) * b[:,V[t + 1]].T, beta[t + 1, :]) # get the overall sum using dot\n",
        "            for i in range(N):\n",
        "                n = alpha[t, i] * a[i,:] * b[:, V[t + 1]].T * beta[t + 1, :].T    # just the curr term\n",
        "                xi[i,:,t] = n / d\n",
        "\n",
        "        gamma = np.sum(xi, axis=1)                                      # gamma is sum of xi along j's (axis=1)\n",
        "\n",
        "        a = np.sum(xi, axis=2) / np.sum(gamma, axis=1).reshape((-1, 1)) # get re-estimated transition probabilities\n",
        "\n",
        "        gamma = np.hstack((gamma, np.sum(xi[:, :, T - 2], axis=0).reshape((-1, 1)))) \n",
        "        \n",
        "        d = np.sum(gamma, axis=1)\n",
        "        for l in range(M):\n",
        "            b[:, l] = np.sum(gamma[:, V == l], axis=1)\n",
        "            \n",
        "        b = (b/d.reshape((-1, 1)))\n",
        "\n",
        "\n",
        "    return (a, b)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H6eaCtNEG3FQ",
        "outputId": "143855ca-81f4-4bfb-97f9-5a40596c15eb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "a =  [[0.53816345 0.46183655]\n",
            " [0.48664443 0.51335557]]\n",
            "b =  [[0.16277513 0.26258073 0.57464414]\n",
            " [0.2514996  0.27780971 0.47069069]]\n"
          ]
        }
      ],
      "source": [
        "\n",
        "data = pd.read_csv('data_python.csv')\n",
        "\n",
        "V = data['Visible'].values\n",
        "\n",
        "# Transition Probabilities\n",
        "a = np.ones((2, 2))\n",
        "a = a / np.sum(a, axis=1)\n",
        "\n",
        "# Emission Probabilities\n",
        "b = np.array(((1, 3, 5), (2, 4, 6)))\n",
        "b = b / np.sum(b, axis=1).reshape((-1, 1))\n",
        "\n",
        "# Equal Probabilities for the initial distribution\n",
        "initial_distribution = np.array((0.5, 0.5))\n",
        "\n",
        "a,b = baum_welch(V, a, b, initial_distribution, n_iter=100)\n",
        "print(\"a = \",a)\n",
        "print(\"b = \",b)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A40-TPXZHTHE"
      },
      "source": [
        "### 3. Decoding Problem : Implementation of Viterbi Algorithm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Goal: Most likely state sequence $V^{T}$ for a given $\\lambda$ \n",
        "\n",
        "Let us define an auxiliary variable $\\delta_{t}(i)$, which is the highest probability that partial observation sequence and state sequence up to t=t can have, when the current state is i. Then,\n",
        "\n",
        "\\begin{align*}\n",
        "\n",
        "\\delta_{t}(i) & = \\max_{q_{1}, q_{2}, ..., q_{t-1}} p(q_{1}, q_{2}, ..., q_{t-1}, q_{t} = i , v_{1}, v_{2}, ... , v_{t-1} | \\lambda) \\\\\n",
        "\\delta_{t+1}(i) & = b_{j}(v_{t+1}) {{\\Large[}\\max_{1 \\leq i \\leq N} \\delta_{t}(i)a_{ij}{\\Large]}} \\\\\n",
        "\\delta_{1}(j) & = \\pi_{j} b_{j}(v_{1})\n",
        "\n",
        "\\end{align*}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "def viterbi(V, a, b, initial_distribution):\n",
        "    T = V.shape[0]\n",
        "    N = a.shape[0]\n",
        "    delta = np.zeros((T, N))\n",
        "\n",
        "    delta[0, :] = np.log(initial_distribution * b[:, V[0]])\n",
        "    prev = np.zeros((T - 1, N))\n",
        "\n",
        "    for t in range(1, T):\n",
        "        for i in range(N):\n",
        "            p = delta[t - 1] + np.log(a[:, i]) + np.log(b[i, V[t]]) # log of the forward probability to avoid underflow\n",
        "            prev[t - 1, i] = np.argmax(p)                           # find the state that maximizes delta_i_t. get index,value\n",
        "            delta[t, i] = np.max(p)\n",
        "\n",
        "    seq = np.zeros(T)\n",
        "    last = np.argmax(delta[T - 1, :])       # get argmax of probs at T-1\n",
        "    seq[0] = last\n",
        "\n",
        "    ind = 1\n",
        "    for i in range(T - 2, -1, -1):\n",
        "        seq[ind] = prev[i, int(last)]\n",
        "        last = prev[i, int(last)]\n",
        "        ind += 1\n",
        "\n",
        "    seq = np.flip(seq, axis=0)              # flip since backtracking\n",
        "\n",
        "    result = []\n",
        "    for ele in seq:\n",
        "        if ele == 0:\n",
        "            result.append(\"A\")\n",
        "        else:\n",
        "            result.append(\"B\")\n",
        "    return result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nBG74dknHYgM",
        "outputId": "1ac04c53-fea0-4047-aae0-768c6c6e0780"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['B', 'B', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'B', 'B', 'B', 'B', 'A', 'A', 'A', 'B', 'A', 'B', 'B', 'A', 'A', 'A', 'B', 'A', 'B', 'A', 'A', 'B', 'A', 'A', 'A', 'B', 'B', 'B', 'B', 'B', 'A', 'A', 'A', 'A', 'B', 'A', 'A', 'A', 'A', 'A', 'B', 'B', 'B', 'B', 'A', 'A', 'A', 'B', 'B', 'B', 'B', 'A', 'B', 'B', 'B', 'B', 'A', 'B', 'B', 'A', 'B', 'B', 'B', 'A', 'B', 'B', 'B', 'A', 'B', 'B', 'B', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'B', 'B', 'B', 'B', 'B', 'B', 'A', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'A', 'B', 'A', 'A', 'B', 'B', 'B', 'B', 'B', 'A', 'A', 'B', 'A', 'B', 'B', 'B', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'B', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'B', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'B', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'B', 'A', 'B', 'B', 'A', 'B', 'B', 'B', 'B', 'B', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'B', 'B', 'B', 'B', 'A', 'B', 'B', 'A', 'A', 'A', 'A', 'A', 'A', 'B', 'B', 'B', 'A', 'A', 'A', 'B', 'B', 'B', 'B', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'B', 'A', 'B', 'B', 'B', 'A', 'B', 'B', 'A', 'A', 'A', 'B', 'B', 'B', 'B', 'B', 'A', 'A', 'A', 'B', 'B', 'A', 'B', 'A', 'A', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'A', 'A', 'B', 'B', 'A', 'A', 'A', 'B', 'B', 'B', 'A', 'A', 'B', 'B', 'A', 'A', 'A', 'A', 'A', 'A', 'B', 'B', 'A', 'A', 'B', 'A', 'B', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'B', 'B', 'B', 'A', 'A', 'A', 'A', 'A', 'B', 'B', 'A', 'B', 'B', 'B', 'B', 'B', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'B', 'A', 'B', 'B', 'A', 'A', 'A', 'A', 'A', 'A', 'B', 'B', 'B', 'B', 'B', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'B', 'B', 'A', 'B', 'B', 'A', 'A', 'A', 'B', 'A', 'B', 'B', 'A', 'A', 'B', 'A', 'A', 'B', 'A', 'A', 'A', 'A', 'B', 'A', 'A', 'A', 'A', 'A', 'B', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'B', 'B', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'B', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'B', 'A', 'A', 'A', 'A', 'B', 'B', 'B', 'B', 'A', 'B', 'B', 'A', 'A', 'A', 'B', 'B', 'B', 'B', 'A', 'A', 'A', 'A', 'A', 'B', 'A', 'A', 'A', 'A', 'B', 'B', 'A', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'A', 'B', 'B', 'B', 'A', 'A', 'A', 'A', 'B', 'B', 'B', 'A', 'A', 'A', 'A', 'A', 'A', 'A']\n"
          ]
        }
      ],
      "source": [
        "data = pd.read_csv('data_python.csv')\n",
        "\n",
        "V = data['Visible'].values\n",
        "\n",
        "# Transition Probabilities\n",
        "a = np.ones((2, 2))\n",
        "a = a / np.sum(a, axis=1)\n",
        "\n",
        "# Emission Probabilities\n",
        "b = np.array(((1, 3, 5), (2, 4, 6)))\n",
        "b = b / np.sum(b, axis=1).reshape((-1, 1))\n",
        "\n",
        "# Equal Probabilities for the initial distribution\n",
        "initial_distribution = np.array((0.5, 0.5))\n",
        "\n",
        "a, b = baum_welch(V, a, b, initial_distribution, n_iter=100)\n",
        "\n",
        "result = viterbi(V, a, b, initial_distribution)\n",
        "\n",
        "print(result)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6wTG6SkvKOHl"
      },
      "source": [
        "4. Use the built-in **hmmlearn** package to fit the data and generate the result using the decoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "R1iy8tj4KrwF"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: hmmlearn==0.2.7 in c:\\users\\bsidd\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (0.2.7)\n",
            "Requirement already satisfied: numpy>=1.10 in c:\\users\\bsidd\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from hmmlearn==0.2.7) (1.21.2)\n",
            "Requirement already satisfied: scipy>=0.19 in c:\\users\\bsidd\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from hmmlearn==0.2.7) (1.7.1)\n",
            "Requirement already satisfied: scikit-learn>=0.16 in c:\\users\\bsidd\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from hmmlearn==0.2.7) (1.0.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\bsidd\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from scikit-learn>=0.16->hmmlearn==0.2.7) (3.1.0)\n",
            "Requirement already satisfied: joblib>=0.11 in c:\\users\\bsidd\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from scikit-learn>=0.16->hmmlearn==0.2.7) (1.1.0)\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING: You are using pip version 22.0.4; however, version 22.3 is available.\n",
            "You should consider upgrading via the 'c:\\Users\\bsidd\\AppData\\Local\\Programs\\Python\\Python39\\python.exe -m pip install --upgrade pip' command.\n"
          ]
        }
      ],
      "source": [
        "%pip install hmmlearn==0.2.7"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "Pw-zo9opLLlh"
      },
      "outputs": [],
      "source": [
        "## Write your code here\n",
        "from hmmlearn import hmm\n",
        "import numpy as np\n",
        "import math\n",
        "\n",
        "data = pd.read_csv('data_python.csv')\n",
        "V = data['Visible'].values\n",
        "\n",
        "model = hmm.MultinomialHMM(n_components=2)\n",
        "model.startprob_ = np.array([0.5, 0.5])\n",
        "model.transmat_ = np.array([[0.5, 0.5],\n",
        "                            [0.5, 0.5]])\n",
        "model.emissionprob_ = np.array([[0.1, 0.3, 0.6],\n",
        "                                [0.2, 0.3 ,0.5]])\n",
        "\n",
        "_, sequence = model.decode((np.array(V).reshape(-1,1)).transpose())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['B', 'B', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'B', 'A', 'A', 'A', 'A', 'B', 'B', 'B', 'B', 'A', 'B', 'A', 'B', 'A', 'B', 'B', 'A', 'B', 'A', 'B', 'A', 'B', 'A', 'A', 'B', 'A', 'A', 'A', 'B', 'B', 'B', 'B', 'B', 'A', 'A', 'A', 'A', 'B', 'A', 'A', 'A', 'B', 'A', 'B', 'B', 'B', 'B', 'A', 'B', 'A', 'B', 'B', 'B', 'B', 'A', 'B', 'B', 'B', 'B', 'A', 'B', 'B', 'A', 'B', 'B', 'B', 'A', 'B', 'B', 'B', 'A', 'B', 'B', 'B', 'A', 'B', 'A', 'B', 'B', 'A', 'B', 'A', 'A', 'A', 'B', 'A', 'B', 'A', 'B', 'B', 'B', 'A', 'A', 'B', 'A', 'A', 'B', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'B', 'B', 'B', 'B', 'B', 'B', 'A', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'A', 'B', 'A', 'A', 'B', 'B', 'B', 'B', 'B', 'A', 'A', 'B', 'A', 'B', 'B', 'B', 'A', 'A', 'A', 'A', 'A', 'B', 'A', 'A', 'A', 'A', 'A', 'B', 'A', 'B', 'A', 'B', 'A', 'B', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'B', 'A', 'B', 'A', 'B', 'B', 'B', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'B', 'A', 'A', 'A', 'A', 'A', 'B', 'A', 'B', 'A', 'B', 'A', 'B', 'A', 'B', 'B', 'A', 'B', 'B', 'B', 'B', 'B', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'B', 'B', 'B', 'B', 'A', 'B', 'B', 'A', 'A', 'B', 'A', 'A', 'A', 'B', 'B', 'B', 'A', 'A', 'A', 'B', 'B', 'B', 'B', 'A', 'A', 'B', 'A', 'A', 'A', 'B', 'A', 'A', 'A', 'A', 'B', 'A', 'B', 'B', 'B', 'A', 'B', 'B', 'A', 'A', 'A', 'B', 'B', 'B', 'B', 'B', 'A', 'B', 'A', 'B', 'B', 'A', 'B', 'A', 'A', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'A', 'A', 'B', 'B', 'A', 'A', 'A', 'B', 'B', 'B', 'A', 'A', 'B', 'B', 'A', 'A', 'A', 'A', 'A', 'A', 'B', 'B', 'A', 'A', 'B', 'A', 'B', 'A', 'A', 'A', 'B', 'A', 'A', 'A', 'B', 'B', 'B', 'B', 'A', 'B', 'B', 'B', 'A', 'A', 'B', 'B', 'A', 'B', 'B', 'A', 'B', 'B', 'B', 'B', 'B', 'A', 'B', 'A', 'B', 'A', 'A', 'A', 'B', 'A', 'B', 'B', 'A', 'A', 'A', 'A', 'A', 'A', 'B', 'B', 'B', 'B', 'B', 'A', 'B', 'A', 'A', 'A', 'A', 'A', 'B', 'B', 'A', 'A', 'A', 'A', 'A', 'A', 'B', 'B', 'A', 'B', 'B', 'A', 'B', 'A', 'B', 'A', 'B', 'B', 'A', 'A', 'B', 'A', 'A', 'B', 'A', 'A', 'A', 'A', 'B', 'A', 'A', 'A', 'B', 'A', 'B', 'A', 'B', 'A', 'A', 'A', 'B', 'A', 'A', 'A', 'B', 'B', 'A', 'B', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'B', 'A', 'A', 'A', 'B', 'A', 'A', 'B', 'A', 'A', 'A', 'A', 'B', 'A', 'B', 'A', 'B', 'A', 'A', 'B', 'B', 'B', 'B', 'A', 'B', 'B', 'A', 'A', 'A', 'B', 'B', 'B', 'B', 'A', 'B', 'A', 'A', 'A', 'B', 'A', 'B', 'A', 'A', 'B', 'B', 'A', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'A', 'B', 'B', 'B', 'A', 'B', 'A', 'A', 'B', 'B', 'B', 'A', 'B', 'B', 'A', 'A', 'B', 'A']\n"
          ]
        }
      ],
      "source": [
        "out = []\n",
        "for i in sequence:\n",
        "    if i == 1:\n",
        "        i = \"B\"\n",
        "    else:\n",
        "        i = \"A\"\n",
        "    out.append(i)\n",
        "print(out)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "name": "Lab_11_Hidden_Markov_Model.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.9.0 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.0"
    },
    "vscode": {
      "interpreter": {
        "hash": "5070ada8c704805c7dd85c5b71cd19812da32b57520493b705b2d0b936990ac0"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
